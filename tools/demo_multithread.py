# For licensing see accompanying LICENSE file.
# Copyright (C) 2025 Apple Inc. All Rights Reserved.
import os
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

import argparse
import glob
import itertools
import numpy as np
import torch
import torchvision
import sys
import uuid
import time
import json
import threading
from datetime import datetime
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from queue import Queue
from pathlib import Path
from PIL import Image
from scipy.spatial.transform import Rotation

try:
    import open3d as o3d
    OPEN3D_AVAILABLE = True
    print("âœ… Open3Då¯ç”¨ï¼Œå°†ä½¿ç”¨ä½“ç´ ä¸‹é‡‡æ ·åŠŸèƒ½")
except ImportError:
    OPEN3D_AVAILABLE = False
    print("âš ï¸ Open3Dä¸å¯ç”¨ï¼Œå°†è·³è¿‡ä½“ç´ ä¸‹é‡‡æ ·")
    o3d = None

from cubifyanything.batching import Sensors
from cubifyanything.boxes import GeneralInstance3DBoxes
from cubifyanything.capture_stream import CaptureDataset
from cubifyanything.color import random_color
from cubifyanything.cubify_transformer import make_cubify_transformer
from cubifyanything.dataset import CubifyAnythingDataset
from cubifyanything.instances import Instances3D
from cubifyanything.preprocessor import Augmentor, Preprocessor

# å¯¼å…¥åŸå§‹ä»£ç ä¸­çš„å·¥å…·å‡½æ•°
from demo_no_rerun import (
    TimingStats, move_device_like, move_to_current_device, 
    move_input_to_current_device, save_pointcloud_ply,
    downsample_pointcloud_with_open3d, compute_nocs_with_bbox_orientation,
    points_in_box, get_camera_coords, unproject
)

class MultiThreadProcessor:
    """å¤šçº¿ç¨‹å¤„ç†å™¨ç±»"""
    def __init__(self, max_workers=4, max_nocs_workers=2, max_pointcloud_workers=2):
        self.max_workers = max_workers
        self.max_nocs_workers = max_nocs_workers
        self.max_pointcloud_workers = max_pointcloud_workers
        self.timing_stats = TimingStats()
        self.lock = threading.Lock()
        
    def process_multiple_scenes(self, data_root, output_root, model_path=None, 
                              score_thresh=0.25, viz_only=False, 
                              every_nth_frame=None, voxel_sizes=[0.004], 
                              splits=None):
        """å¤„ç†å¤šä¸ªåœºæ™¯çš„ä¸»å‡½æ•°"""
        print(f"ğŸš€ å¼€å§‹å¤šçº¿ç¨‹å¤„ç†åœºæ™¯")
        print(f"ğŸ“ æ•°æ®æ ¹ç›®å½•: {data_root}")
        print(f"ğŸ“ è¾“å‡ºæ ¹ç›®å½•: {output_root}")
        print(f"ğŸ§µ åœºæ™¯å¤„ç†çº¿ç¨‹æ•°: {self.max_workers}")
        print(f"ğŸ§µ NOCSç”Ÿæˆçº¿ç¨‹æ•°: {self.max_nocs_workers}")
        print(f"ğŸ§µ ç‚¹äº‘ä¿å­˜çº¿ç¨‹æ•°: {self.max_pointcloud_workers}")
        
        # æ‰«ææ‰€æœ‰åœºæ™¯æ–‡ä»¶
        scene_files = self._discover_scene_files(data_root, splits)
        print(f"ğŸ“Š å‘ç° {len(scene_files)} ä¸ªåœºæ™¯æ–‡ä»¶")
        
        # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœä¸æ˜¯ä»…å¯è§†åŒ–æ¨¡å¼ï¼‰
        model, augmentor, preprocessor = None, None, None
        if not viz_only:
            model, augmentor, preprocessor = self._load_model(model_path)
        
        self.timing_stats.start_total_timer()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†åœºæ™¯
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_scene = {
                executor.submit(
                    self._process_single_scene, 
                    scene_file, output_root, model, augmentor, preprocessor,
                    score_thresh, viz_only, every_nth_frame, voxel_sizes
                ): scene_file for scene_file in scene_files
            }
            
            completed_count = 0
            for future in as_completed(future_to_scene):
                scene_file = future_to_scene[future]
                try:
                    result = future.result()
                    completed_count += 1
                    print(f"âœ… å®Œæˆåœºæ™¯ {completed_count}/{len(scene_files)}: {Path(scene_file).name}")
                    if result:
                        print(f"   å¤„ç†äº† {result['frames']} å¸§ï¼Œ{result['instances']} ä¸ªå®ä¾‹")
                except Exception as exc:
                    print(f"âŒ åœºæ™¯å¤„ç†å¤±è´¥ {Path(scene_file).name}: {exc}")
        
        total_time = self.timing_stats.end_total_timer()
        print(f"\nğŸ‰ æ‰€æœ‰åœºæ™¯å¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_root}")
        self.timing_stats.print_summary()
        
    def _discover_scene_files(self, data_root, splits=None):
        """å‘ç°æ‰€æœ‰åœºæ™¯æ–‡ä»¶"""
        scene_files = []
        
        # é»˜è®¤å¤„ç†æ‰€æœ‰split
        if splits is None:
            splits = ['train', 'val']
        elif isinstance(splits, str):
            splits = [splits]
        
        print(f"ğŸ¯ å¤„ç†æ•°æ®é›†åˆ’åˆ†: {splits}")
        
        # æ£€æŸ¥æŒ‡å®šçš„splitç›®å½•
        for split in splits:
            split_path = os.path.join(data_root, split)
            if os.path.exists(split_path):
                tar_files = glob.glob(os.path.join(split_path, "ca1m-*.tar"))
                scene_files.extend(tar_files)
                print(f"ğŸ“‚ {split}: å‘ç° {len(tar_files)} ä¸ªåœºæ™¯æ–‡ä»¶")
            else:
                print(f"âš ï¸ {split} ç›®å½•ä¸å­˜åœ¨: {split_path}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ä¸”åªæœ‰ä¸€ä¸ªsplitï¼Œå°è¯•åœ¨æ ¹ç›®å½•æŸ¥æ‰¾
        if not scene_files and len(splits) == 1:
            tar_files = glob.glob(os.path.join(data_root, "ca1m-*.tar"))
            if tar_files:
                scene_files.extend(tar_files)
                print(f"ğŸ“‚ æ ¹ç›®å½•: å‘ç° {len(tar_files)} ä¸ªåœºæ™¯æ–‡ä»¶")
            
        return sorted(scene_files)
    
    def _load_model(self, model_path):
        """åŠ è½½æ¨¡å‹"""
        print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
        checkpoint = torch.load(model_path, map_location="cpu")["model"]
        
        backbone_embedding_dimension = checkpoint["backbone.0.patch_embed.proj.weight"].shape[0]
        is_depth_model = any(k.startswith("backbone.0.patch_embed_depth.") for k in checkpoint.keys())
        
        model = make_cubify_transformer(dimension=backbone_embedding_dimension, depth_model=is_depth_model).eval()
        model.load_state_dict(checkpoint)
        model = model.to("cuda" if torch.cuda.is_available() else "cpu")
        
        augmentor = Augmentor(("wide/image", "wide/depth") if is_depth_model else ("wide/image",))
        preprocessor = Preprocessor()
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        return model, augmentor, preprocessor
    
    def _process_single_scene(self, scene_file, output_root, model, augmentor, preprocessor,
                            score_thresh, viz_only, every_nth_frame, voxel_sizes):
        """å¤„ç†å•ä¸ªåœºæ™¯"""
        scene_path = Path(scene_file)
        scene_name = scene_path.stem  # å»æ‰.taråç¼€
        
        # ç¡®å®šè¾“å‡ºç›®å½•ç»“æ„
        split = scene_path.parent.name if scene_path.parent.name in ['train', 'val'] else 'unknown'
        scene_output_dir = os.path.join(output_root, split, scene_name)
        
        print(f"ğŸ¬ å¼€å§‹å¤„ç†åœºæ™¯: {scene_name} (æ¥è‡ª {split})")
        print(f"ğŸ“ åœºæ™¯è¾“å‡ºç›®å½•: {scene_output_dir}")
        
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
            os.makedirs(scene_output_dir, exist_ok=True)
            nocs_dir = os.path.join(scene_output_dir, "nocs_images")
            objects_dir = os.path.join(scene_output_dir, "objects")
            os.makedirs(nocs_dir, exist_ok=True)
            os.makedirs(objects_dir, exist_ok=True)
            
            # åˆ›å»ºæ•°æ®é›†
            dataset = CubifyAnythingDataset(
                [scene_path.as_uri()],
                yield_world_instances=True,  # æ€»æ˜¯è·å–ä¸–ç•Œåæ ‡ç³»instances
                load_arkit_depth=True,
                use_cache=False
            )
            
            if every_nth_frame is not None:
                dataset = itertools.islice(dataset, 0, None, every_nth_frame)
            
            # å¤„ç†åœºæ™¯æ•°æ®
            if viz_only:
                result = self._process_scene_visualization_only(
                    dataset, scene_output_dir, nocs_dir, objects_dir, voxel_sizes)
            else:
                result = self._process_scene_with_model(
                    dataset, model, augmentor, preprocessor, 
                    scene_output_dir, nocs_dir, objects_dir, 
                    score_thresh, voxel_sizes)
            
            print(f"âœ… åœºæ™¯å¤„ç†å®Œæˆ: {scene_name}")
            return result
            
        except Exception as e:
            print(f"âŒ åœºæ™¯å¤„ç†å¤±è´¥ {scene_name}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _process_scene_with_model(self, dataset, model, augmentor, preprocessor,
                                scene_output_dir, nocs_dir, objects_dir, 
                                score_thresh, voxel_sizes):
        """ä½¿ç”¨æ¨¡å‹å¤„ç†åœºæ™¯"""
        frame_count = 0
        instance_pointclouds = {}
        all_pred_boxes = []
        scene_bbox_info = None
        device = model.pixel_mean
        
        # NOCSå’Œç‚¹äº‘å¤„ç†é˜Ÿåˆ—
        nocs_queue = Queue()
        pointcloud_queue = Queue()
        
        # å¯åŠ¨NOCSå¤„ç†çº¿ç¨‹
        nocs_threads = []
        for i in range(self.max_nocs_workers):
            thread = threading.Thread(
                target=self._nocs_worker, 
                args=(nocs_queue, nocs_dir),
                daemon=True
            )
            thread.start()
            nocs_threads.append(thread)
        
        for sample in dataset:
            frame_count += 1
            
            # æå–åœºæ™¯bboxä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€å¸§æˆ–worldå¸§ï¼‰
            if scene_bbox_info is None and "world" in sample:
                scene_bbox_info = self._extract_scene_bbox_info(sample)
            
            if frame_count % 10 == 0:
                print(f"ğŸ“Š å¤„ç†å¸§ {frame_count}...")
            
            # æ•°æ®é¢„å¤„ç†
            image = np.moveaxis(sample["wide"]["image"][-1].numpy(), 0, -1)
            packaged = augmentor.package(sample)
            packaged = move_input_to_current_device(packaged, device)
            packaged = preprocessor.preprocess([packaged])
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                pred_instances = model(packaged)[0]
            pred_instances = pred_instances[pred_instances.scores >= score_thresh]
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            if len(pred_instances) > 0:
                boxes_3d = pred_instances.get("pred_boxes_3d")
                all_pred_boxes.append({
                    'centers': boxes_3d.gravity_center.cpu().numpy(),
                    'sizes': boxes_3d.dims.cpu().numpy(),
                    'rotations': boxes_3d.R.cpu().numpy(),
                    'scores': pred_instances.scores.cpu().numpy(),
                    'frame': frame_count
                })
            
            # å¤„ç†æ·±åº¦æ•°æ®å’ŒNOCSç”Ÿæˆ
            if sample["sensor_info"].has("gt") and "depth" in sample["gt"]:
                self._process_frame_depth_data(
                    sample, image, frame_count, nocs_queue, 
                    instance_pointclouds, scene_bbox_info)
        
        # åœæ­¢NOCSå¤„ç†çº¿ç¨‹
        for _ in range(self.max_nocs_workers):
            nocs_queue.put(None)
        for thread in nocs_threads:
            thread.join()
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        if all_pred_boxes:
            self._save_prediction_results(all_pred_boxes, scene_output_dir)
        
        # ä¿å­˜åœºæ™¯bboxä¿¡æ¯
        if scene_bbox_info:
            self._save_scene_bbox_info(scene_bbox_info, scene_output_dir)
        
        # å¤šçº¿ç¨‹ä¿å­˜å½’ä¸€åŒ–ç‚¹äº‘
        if instance_pointclouds:
            self._save_normalized_pointclouds_multithread(
                instance_pointclouds, objects_dir, voxel_sizes)
        
        return {
            'frames': frame_count,
            'instances': len(instance_pointclouds),
            'predictions': len(all_pred_boxes)
        }
    
    def _process_scene_visualization_only(self, dataset, scene_output_dir, 
                                        nocs_dir, objects_dir, voxel_sizes):
        """ä»…å¯è§†åŒ–å¤„ç†åœºæ™¯"""
        frame_count = 0
        instance_pointclouds = {}
        scene_bbox_info = None
        
        # NOCSå¤„ç†é˜Ÿåˆ—
        nocs_queue = Queue()
        
        # å¯åŠ¨NOCSå¤„ç†çº¿ç¨‹
        nocs_threads = []
        for i in range(self.max_nocs_workers):
            thread = threading.Thread(
                target=self._nocs_worker,
                args=(nocs_queue, nocs_dir),
                daemon=True
            )
            thread.start()
            nocs_threads.append(thread)
        
        for sample in dataset:
            frame_count += 1
            
            # æå–åœºæ™¯bboxä¿¡æ¯
            if scene_bbox_info is None and "world" in sample:
                scene_bbox_info = self._extract_scene_bbox_info(sample)
            
            if frame_count % 10 == 0:
                print(f"ğŸ“Š [VIZ] å¤„ç†å¸§ {frame_count}...")
            
            # å¤„ç†å›¾åƒæ•°æ®
            if "wide" not in sample or "image" not in sample["wide"]:
                continue
                
            image = np.moveaxis(sample["wide"]["image"][-1].numpy(), 0, -1)
            
            # å¤„ç†æ·±åº¦æ•°æ®å’ŒNOCSç”Ÿæˆ
            if "gt" in sample and sample["gt"] is not None and "depth" in sample["gt"]:
                self._process_frame_depth_data(
                    sample, image, frame_count, nocs_queue, 
                    instance_pointclouds, scene_bbox_info)
        
        # åœæ­¢NOCSå¤„ç†çº¿ç¨‹
        for _ in range(self.max_nocs_workers):
            nocs_queue.put(None)
        for thread in nocs_threads:
            thread.join()
        
        # ä¿å­˜åœºæ™¯bboxä¿¡æ¯
        if scene_bbox_info:
            self._save_scene_bbox_info(scene_bbox_info, scene_output_dir)
        
        # å¤šçº¿ç¨‹ä¿å­˜å½’ä¸€åŒ–ç‚¹äº‘
        if instance_pointclouds:
            self._save_normalized_pointclouds_multithread(
                instance_pointclouds, objects_dir, voxel_sizes)
        
        return {
            'frames': frame_count,
            'instances': len(instance_pointclouds)
        }
    
    def _process_frame_depth_data(self, sample, image, frame_count, nocs_queue, 
                                instance_pointclouds, scene_bbox_info):
        """å¤„ç†å•å¸§çš„æ·±åº¦æ•°æ®"""
        depth_gt = sample["gt"]["depth"][-1]
        matched_image = torch.tensor(np.array(Image.fromarray(image).resize((depth_gt.shape[1], depth_gt.shape[0]))))
        
        RT_camera_to_world = sample["sensor_info"].gt.RT[-1]
        xyz, valid = unproject(depth_gt, sample["sensor_info"].gt.depth.K[-1], RT_camera_to_world, max_depth=10.0)
        
        height, width = depth_gt.shape[-2:]
        v_coords, u_coords = torch.meshgrid(torch.arange(height), torch.arange(width), indexing='ij')
        pixel_coords = torch.stack([u_coords, v_coords], dim=-1)
        
        xyzrgb = torch.cat((xyz, matched_image / 255.0), dim=-1)[valid]
        pixel_coords_valid = pixel_coords[valid]
        
        if len(xyzrgb) == 0:
            return
        
        # è·å–GTå®ä¾‹
        world_gt_instances = self._get_world_gt_instances(sample, RT_camera_to_world)
        
        if world_gt_instances is not None and len(world_gt_instances) > 0:
            # æ·»åŠ NOCSç”Ÿæˆä»»åŠ¡åˆ°é˜Ÿåˆ—
            nocs_task = {
                'points_3d': xyzrgb[..., :3].cpu().numpy(),
                'gt_instances': world_gt_instances,
                'image_shape': depth_gt.shape[-2:],
                'pixel_coords': pixel_coords_valid.cpu().numpy(),
                'frame_count': frame_count,
                'camera_K': sample["sensor_info"].wide.image.K[-1].numpy(),
                'camera_RT': RT_camera_to_world.numpy()
            }
            nocs_queue.put(nocs_task)
            
            # æ”¶é›†å®ä¾‹ç‚¹äº‘
            self._collect_instance_pointclouds_threadsafe(
                xyzrgb[..., :3].cpu().numpy(),
                xyzrgb[..., 3:].cpu().numpy(),
                world_gt_instances,
                frame_count,
                instance_pointclouds
            )
    
    def _get_world_gt_instances(self, sample, RT_camera_to_world):
        """è·å–ä¸–ç•Œåæ ‡ç³»çš„GTå®ä¾‹"""
        # ä¼˜å…ˆä½¿ç”¨ä¸–ç•Œåæ ‡ç³»instances
        if "world_instances_3d" in sample and sample["world_instances_3d"] is not None:
            return sample["world_instances_3d"]
        elif "world" in sample and "instances" in sample["world"]:
            return sample["world"]["instances"]
        else:
            # ä»ç›¸æœºåæ ‡ç³»è½¬æ¢
            if "wide" in sample and "instances" in sample["wide"]:
                gt_instances = sample["wide"]["instances"]
                world_gt_instances = gt_instances.clone()
                
                if world_gt_instances.has("gt_boxes_3d"):
                    original_boxes = world_gt_instances.get("gt_boxes_3d")
                    RT_np = RT_camera_to_world.numpy()
                    
                    centers = original_boxes.gravity_center.cpu().numpy()
                    centers_homogeneous = np.concatenate([centers, np.ones((centers.shape[0], 1))], axis=1)
                    transformed_centers = (RT_np @ centers_homogeneous.T).T[:, :3]
                    
                    transformed_R = RT_np[:3, :3] @ original_boxes.R.cpu().numpy()
                    transformed_boxes = GeneralInstance3DBoxes(
                        np.concatenate([transformed_centers, original_boxes.dims.cpu().numpy()], axis=1),
                        transformed_R
                    )
                    
                    world_gt_instances.set("gt_boxes_3d", transformed_boxes)
                    return world_gt_instances
        
        return None
    
    def _nocs_worker(self, nocs_queue, nocs_dir):
        """NOCSç”Ÿæˆå·¥ä½œçº¿ç¨‹"""
        while True:
            task = nocs_queue.get()
            if task is None:
                break
            
            try:
                nocs_image = self._generate_instance_nocs_map(
                    task['points_3d'], task['gt_instances'], 
                    task['camera_K'], task['camera_RT'],
                    task['image_shape'], task['pixel_coords']
                )
                
                if nocs_image is not None:
                    nocs_image_pil = Image.fromarray(nocs_image)
                    nocs_file = os.path.join(nocs_dir, f"frame_{task['frame_count']:04d}_nocs.png")
                    nocs_image_pil.save(nocs_file)
                    
            except Exception as e:
                print(f"âŒ NOCSç”Ÿæˆå¤±è´¥ (å¸§ {task['frame_count']}): {e}")
            finally:
                nocs_queue.task_done()
    
    def _generate_instance_nocs_map(self, points_3d, gt_instances, camera_K, camera_RT, image_shape, pixel_coords=None):
        """ç”Ÿæˆå®ä¾‹NOCSå›¾ï¼ˆå¤šçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰"""
        if gt_instances is None or len(gt_instances) == 0:
            return None
        
        height, width = image_shape
        
        if points_3d.ndim == 2:
            if pixel_coords is None:
                return None
            
            points_3d_reshaped = points_3d
            pixel_u = pixel_coords[:, 0]
            pixel_v = pixel_coords[:, 1]
            
            valid_mask = (pixel_u >= 0) & (pixel_u < width) & (pixel_v >= 0) & (pixel_v < height) & \
                        np.isfinite(points_3d_reshaped).all(axis=1) & (np.linalg.norm(points_3d_reshaped, axis=1) > 0.01)
        else:
            return None
        
        points_3d_valid = points_3d_reshaped[valid_mask]
        pixel_u_valid = pixel_u[valid_mask]
        pixel_v_valid = pixel_v[valid_mask]
        
        if len(points_3d_valid) == 0:
            return None
        
        nocs_image = np.zeros((height, width, 3), dtype=np.float32)
        
        if gt_instances.has("gt_boxes_3d"):
            boxes_3d = gt_instances.get("gt_boxes_3d")
        else:
            return None
            
        corners = boxes_3d.corners.cpu().numpy()
        rotations = boxes_3d.R.cpu().numpy()
        centers = boxes_3d.gravity_center.cpu().numpy()
        dims = boxes_3d.dims.cpu().numpy()
        
        for i in range(len(boxes_3d)):
            box_corners = corners[i]
            box_rotation = rotations[i]
            box_center = centers[i]
            box_dims = dims[i]
            
            in_box_mask = points_in_box(points_3d_valid, box_corners)
            
            point_count = np.sum(in_box_mask)
            
            if point_count > 0:
                instance_points = points_3d_valid[in_box_mask]
                instance_u = pixel_u_valid[in_box_mask]
                instance_v = pixel_v_valid[in_box_mask]
                
                nocs_points, nocs_colors = compute_nocs_with_bbox_orientation(
                    instance_points, box_center, box_rotation, box_dims, i)
                
                nocs_image[instance_v, instance_u] = nocs_colors
        
        if np.any(nocs_image > 0):
            nocs_image_display = (np.clip(nocs_image, 0, 1) * 255).astype(np.uint8)
            return nocs_image_display
        else:
            return None
    
    def _collect_instance_pointclouds_threadsafe(self, points_3d, colors, gt_instances, 
                                               frame_num, instance_pointclouds):
        """çº¿ç¨‹å®‰å…¨çš„å®ä¾‹ç‚¹äº‘æ”¶é›†"""
        if not gt_instances.has("gt_boxes_3d") or not gt_instances.has("gt_ids"):
            return
        
        boxes_3d = gt_instances.get("gt_boxes_3d")
        instance_ids = gt_instances.get("gt_ids")
        corners = boxes_3d.corners.cpu().numpy()
        centers = boxes_3d.gravity_center.cpu().numpy()
        rotations = boxes_3d.R.cpu().numpy()
        dims = boxes_3d.dims.cpu().numpy()
        
        with self.lock:
            for i in range(len(boxes_3d)):
                instance_id = instance_ids[i]
                box_corners = corners[i]
                box_center = centers[i]
                box_rotation = rotations[i]
                box_dims = dims[i]
                
                in_box_mask = points_in_box(points_3d, box_corners)
                point_count = np.sum(in_box_mask)
                
                if point_count > 0:
                    instance_points = points_3d[in_box_mask]
                    instance_colors = colors[in_box_mask]
                    
                    if instance_id not in instance_pointclouds:
                        instance_pointclouds[instance_id] = {
                            'pointcloud_frames': [],
                            'reference_bbox': None
                        }
                    
                    if instance_pointclouds[instance_id]['reference_bbox'] is None:
                        instance_pointclouds[instance_id]['reference_bbox'] = {
                            'center': box_center,
                            'rotation': box_rotation,
                            'dims': box_dims,
                            'reference_frame': frame_num
                        }
                    
                    instance_pointclouds[instance_id]['pointcloud_frames'].append({
                        'points': instance_points,
                        'colors': instance_colors,
                        'frame': frame_num,
                        'current_bbox': {
                            'center': box_center,
                            'rotation': box_rotation,
                            'dims': box_dims
                        }
                    })
    
    def _save_normalized_pointclouds_multithread(self, instance_pointclouds, objects_dir, voxel_sizes):
        """å¤šçº¿ç¨‹ä¿å­˜å½’ä¸€åŒ–ç‚¹äº‘"""
        print(f"ğŸ§µ ä½¿ç”¨ {self.max_pointcloud_workers} ä¸ªçº¿ç¨‹ä¿å­˜å½’ä¸€åŒ–ç‚¹äº‘...")
        
        with ThreadPoolExecutor(max_workers=self.max_pointcloud_workers) as executor:
            future_to_instance = {}
            
            for instance_id, instance_data in instance_pointclouds.items():
                if not instance_data or 'pointcloud_frames' not in instance_data:
                    continue
                
                future = executor.submit(
                    self._save_single_instance_pointcloud,
                    instance_id, instance_data, objects_dir, voxel_sizes
                )
                future_to_instance[future] = instance_id
            
            completed_count = 0
            for future in as_completed(future_to_instance):
                instance_id = future_to_instance[future]
                try:
                    result = future.result()
                    completed_count += 1
                    if result:
                        print(f"ğŸ’¾ ä¿å­˜å®ä¾‹ {completed_count}/{len(future_to_instance)}: {instance_id}")
                except Exception as exc:
                    print(f"âŒ å®ä¾‹ç‚¹äº‘ä¿å­˜å¤±è´¥ {instance_id}: {exc}")
    
    def _save_single_instance_pointcloud(self, instance_id, instance_data, objects_dir, voxel_sizes):
        """ä¿å­˜å•ä¸ªå®ä¾‹çš„å½’ä¸€åŒ–ç‚¹äº‘"""
        pointcloud_frames = instance_data['pointcloud_frames']
        reference_bbox = instance_data['reference_bbox']
        
        if not pointcloud_frames or not reference_bbox:
            return False
        
        # åˆ›å»ºå®ä¾‹ç›®å½•
        instance_dir = os.path.join(objects_dir, str(instance_id))
        os.makedirs(instance_dir, exist_ok=True)
        
        ref_center = reference_bbox['center']
        ref_rotation = reference_bbox['rotation']
        ref_dims = reference_bbox['dims']
        
        all_normalized_points = []
        all_colors = []
        
        # å½’ä¸€åŒ–å¤„ç†
        for frame_data in pointcloud_frames:
            frame_points = frame_data['points']
            frame_colors = frame_data['colors']
            
            normalized_points, normalized_colors = compute_nocs_with_bbox_orientation(
                frame_points, ref_center, ref_rotation, ref_dims, f"{instance_id}")
            
            all_normalized_points.append(normalized_points)
            all_colors.append(frame_colors)
        
        if all_normalized_points:
            # åˆå¹¶æ‰€æœ‰å½’ä¸€åŒ–ç‚¹äº‘
            final_normalized_points = np.concatenate(all_normalized_points, axis=0)
            final_colors = np.concatenate(all_colors, axis=0)
            
            # å¤šçº§ä¸‹é‡‡æ ·ä¿å­˜
            if voxel_sizes:
                for voxel_size in voxel_sizes:
                    downsampled_points, downsampled_colors = downsample_pointcloud_with_open3d(
                        final_normalized_points, final_colors, voxel_size)
                    
                    if voxel_size == voxel_sizes[0]:
                        normalized_file = os.path.join(instance_dir, f"normalized.ply")
                    else:
                        normalized_file = os.path.join(instance_dir, f"normalized_voxel_{voxel_size}.ply")
                    
                    save_pointcloud_ply(downsampled_points, downsampled_colors, normalized_file)
            else:
                normalized_file = os.path.join(instance_dir, f"normalized.ply")
                save_pointcloud_ply(final_normalized_points, final_colors, normalized_file)
            
            # ä¿å­˜å®ä¾‹ä¿¡æ¯
            info_file = os.path.join(instance_dir, "info.json")
            instance_info = {
                'instance_id': int(instance_id) if isinstance(instance_id, (int, np.integer)) else str(instance_id),
                'total_frames': len(pointcloud_frames),
                'total_points': len(final_normalized_points),
                'reference_bbox': {
                    'center': ref_center.tolist() if hasattr(ref_center, 'tolist') else ref_center,
                    'rotation': ref_rotation.tolist() if hasattr(ref_rotation, 'tolist') else ref_rotation,
                    'dims': ref_dims.tolist() if hasattr(ref_dims, 'tolist') else ref_dims,
                    'reference_frame': reference_bbox['reference_frame']
                },
                'normalized_range': {
                    'x': [float(final_normalized_points[:, 0].min()), float(final_normalized_points[:, 0].max())],
                    'y': [float(final_normalized_points[:, 1].min()), float(final_normalized_points[:, 1].max())],
                    'z': [float(final_normalized_points[:, 2].min()), float(final_normalized_points[:, 2].max())]
                }
            }
            
            with open(info_file, 'w') as f:
                json.dump(instance_info, f, indent=2)
            
            return True
        
        return False
    
    def _extract_scene_bbox_info(self, sample):
        """æå–åœºæ™¯bboxä¿¡æ¯"""
        if "world" in sample and "instances" in sample["world"]:
            world_instances = sample["world"]["instances"]
            
            if world_instances.has("gt_boxes_3d") and world_instances.has("gt_ids"):
                boxes_3d = world_instances.get("gt_boxes_3d")
                instance_ids = world_instances.get("gt_ids")
                
                scene_info = {
                    'total_instances': len(boxes_3d),
                    'instances': []
                }
                
                centers = boxes_3d.gravity_center.cpu().numpy()
                dims = boxes_3d.dims.cpu().numpy()
                rotations = boxes_3d.R.cpu().numpy()
                
                for i in range(len(boxes_3d)):
                    instance_info = {
                        'id': int(instance_ids[i]) if isinstance(instance_ids[i], (int, np.integer)) else str(instance_ids[i]),
                        'center': centers[i].tolist(),
                        'dimensions': dims[i].tolist(),
                        'rotation': rotations[i].tolist()
                    }
                    
                    if world_instances.has("gt_names"):
                        names = world_instances.get("gt_names")
                        if i < len(names):
                            instance_info['category'] = names[i]
                    
                    scene_info['instances'].append(instance_info)
                
                return scene_info
        
        return None
    
    def _save_scene_bbox_info(self, scene_bbox_info, scene_output_dir):
        """ä¿å­˜åœºæ™¯bboxä¿¡æ¯"""
        if scene_bbox_info:
            bbox_file = os.path.join(scene_output_dir, "scene_bbox_info.json")
            with open(bbox_file, 'w') as f:
                json.dump(scene_bbox_info, f, indent=2)
            print(f"ğŸ’¾ ä¿å­˜åœºæ™¯bboxä¿¡æ¯: {bbox_file}")
    
    def _save_prediction_results(self, all_pred_boxes, scene_output_dir):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        if all_pred_boxes:
            predictions_file = os.path.join(scene_output_dir, "predictions.json")
            
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            serializable_predictions = []
            for box_data in all_pred_boxes:
                pred_data = {
                    'frame': int(box_data['frame']),
                    'num_boxes': len(box_data['centers']),
                    'centers': box_data['centers'].tolist(),
                    'sizes': box_data['sizes'].tolist(),
                    'rotations': box_data['rotations'].tolist(),
                    'scores': box_data['scores'].tolist()
                }
                serializable_predictions.append(pred_data)
            
            with open(predictions_file, 'w') as f:
                json.dump({
                    'total_frames': len(all_pred_boxes),
                    'total_predictions': sum(len(box['centers']) for box in all_pred_boxes),
                    'predictions': serializable_predictions
                }, f, indent=2)
            
            print(f"ğŸ’¾ ä¿å­˜é¢„æµ‹ç»“æœ: {predictions_file}")


def main():
    parser = argparse.ArgumentParser(description="å¤šçº¿ç¨‹å¤„ç†CA-1Mæ•°æ®é›†")
    
    parser.add_argument("--data-root", default="/baai-cwm-vepfs/cwm/chongjie.ye/data/CA-1M/data", 
                       help="æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--output-root", default="/baai-cwm-vepfs/cwm/chongjie.ye/data/CA-1M_output", 
                       help="è¾“å‡ºæ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--model-path", help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--viz-only", default=False, action="store_true", 
                       help="ä»…å¯è§†åŒ–æ¨¡å¼ï¼Œè·³è¿‡æ¨¡å‹åŠ è½½")
    parser.add_argument("--score-thresh", default=0.25, type=float, 
                       help="æ£€æµ‹é˜ˆå€¼")
    parser.add_argument("--every-nth-frame", default=None, type=int, 
                       help="æ¯Nå¸§å¤„ç†ä¸€æ¬¡")
    parser.add_argument("--max-workers", default=4, type=int, 
                       help="åœºæ™¯å¤„ç†æœ€å¤§çº¿ç¨‹æ•°")
    parser.add_argument("--max-nocs-workers", default=2, type=int, 
                       help="NOCSç”Ÿæˆæœ€å¤§çº¿ç¨‹æ•°")
    parser.add_argument("--max-pointcloud-workers", default=2, type=int, 
                       help="ç‚¹äº‘ä¿å­˜æœ€å¤§çº¿ç¨‹æ•°")
    parser.add_argument("--voxel-sizes", nargs="+", type=float, default=[0.004], 
                       help="ä½“ç´ ä¸‹é‡‡æ ·å°ºå¯¸åˆ—è¡¨")
    parser.add_argument("--disable-downsampling", default=False, action="store_true", 
                       help="ç¦ç”¨ä½“ç´ ä¸‹é‡‡æ ·")
    parser.add_argument("--splits", nargs="+", default=None,
                       help="æŒ‡å®šè¦å¤„ç†çš„æ•°æ®é›†åˆ’åˆ† (train, val æˆ– both)")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¤šçº¿ç¨‹CA-1Mæ•°æ®é›†å¤„ç†å™¨")
    print("=" * 60)
    print(f"ğŸ“ æ•°æ®æ ¹ç›®å½•: {args.data_root}")
    print(f"ğŸ“ è¾“å‡ºæ ¹ç›®å½•: {args.output_root}")
    print(f"ğŸ§µ æœ€å¤§å·¥ä½œçº¿ç¨‹: åœºæ™¯={args.max_workers}, NOCS={args.max_nocs_workers}, ç‚¹äº‘={args.max_pointcloud_workers}")
    
    if not args.viz_only and args.model_path is None:
        print("âŒ é”™è¯¯: éå¯è§†åŒ–æ¨¡å¼éœ€è¦æä¾›æ¨¡å‹è·¯å¾„")
        sys.exit(1)
    
    if args.disable_downsampling:
        args.voxel_sizes = []
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = MultiThreadProcessor(
        max_workers=args.max_workers,
        max_nocs_workers=args.max_nocs_workers,
        max_pointcloud_workers=args.max_pointcloud_workers
    )
    
    # å¼€å§‹å¤„ç†
    processor.process_multiple_scenes(
        data_root=args.data_root,
        output_root=args.output_root,
        model_path=args.model_path,
        score_thresh=args.score_thresh,
        viz_only=args.viz_only,
        every_nth_frame=args.every_nth_frame,
        voxel_sizes=args.voxel_sizes,
        splits=args.splits
    )


if __name__ == "__main__":
    main()
